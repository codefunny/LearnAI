{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cH-F06ZvF3q",
        "outputId": "735a65c7-510d-43e0-d971-9011cd8dbacd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0+cu124\n",
            "False\n",
            "tensor([[0.6312, 0.6904, 0.8570],\n",
            "        [0.8477, 0.0056, 0.2654],\n",
            "        [0.9901, 0.5780, 0.0797],\n",
            "        [0.7979, 0.1772, 0.6092],\n",
            "        [0.4524, 0.4592, 0.9932]])\n",
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]]) tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]]) tensor([[ 1.3319, -0.3214,  0.9812],\n",
            "        [ 0.1165, -1.5420, -2.0866]])\n",
            "tensor([[ 2.3319,  0.6786,  1.9812],\n",
            "        [ 1.1165, -0.5420, -1.0866]])\n",
            "tensor([[ 1.3319, -0.3214,  0.9812],\n",
            "        [ 0.1165, -1.5420, -2.0866]])\n",
            "torch.Size([2, 3])\n",
            "tensor([1, 4, 7]) tensor([1.0000, 2.6000, 4.2000, 5.8000, 7.4000, 9.0000])\n",
            "tensor([[1, 2, 3],\n",
            "        [2, 3, 4],\n",
            "        [4, 5, 6]])\n",
            "tensor([[1, 2, 4],\n",
            "        [2, 3, 5],\n",
            "        [3, 4, 6]])\n",
            "tensor([2.])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "x = torch.rand(5,3) #创建一个服从均匀分布的随机张量，值在 [0, 1)。\n",
        "print(x)\n",
        "\n",
        "a = torch.zeros(2,3)\n",
        "b = torch.ones(2,3)\n",
        "c = torch.randn(2,3) #创建一个服从正态分布的随机张量，均值为 0，标准差为 1。\n",
        "d = torch.arange(1,10,3) #创建一个一维序列张量,start,end,step\n",
        "e = torch.linspace(1,9,6) #创建一个在指定范围内等间隔的序列张量。\n",
        "f = torch.eye(3) #创建一个单位矩阵（对角线为 1，其他为 0）。\n",
        "g = torch.tensor([[1,2,3],[2,3,4],[4,5,6]])\n",
        "print(a,b,c)\n",
        "print(a+b+c)\n",
        "print(b * c)\n",
        "print(a.shape)\n",
        "print(d,e)\n",
        "print(g)\n",
        "print(g.mT) # x.mT is equivalent to x.transpose(-2, -1).\n",
        "\n",
        "tensor_requires_grad = torch.tensor([1.0], requires_grad=True)\n",
        "\n",
        "# 进行一些操作\n",
        "tensor_result = tensor_requires_grad * 2\n",
        "\n",
        "# 计算梯度\n",
        "tensor_result.backward()\n",
        "print(tensor_requires_grad.grad)  # 输出梯度"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(SimpleNN,self).__init__()\n",
        "    self.fc1 = nn.Linear(2,2)\n",
        "    self.fc2 = nn.Linear(2,1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.relu(self.fc1(x))\n",
        "    x = self.fc2(x)\n",
        "    return x\n",
        "\n",
        "model = SimpleNN()\n",
        "\n",
        "# 定义损失函数和优化器\n",
        "criterion = nn.MSELoss() # 均方误差损失函数\n",
        "optimizer = optim.Adam(model.parameters(),lr=0.001) #Adam优化器\n",
        "\n",
        "X = torch.randn(10,2) #10个样本，2个特征\n",
        "Y = torch.randn(10,1) # 10个目标值\n",
        "\n",
        "# 训练循环\n",
        "for epoch in range(100): #训练100轮\n",
        "  optimizer.zero_grad() #清空之前的梯度\n",
        "  output = model(X) #前向传播\n",
        "  loss = criterion(output, Y) #计算损失\n",
        "  loss.backward() #反向传播\n",
        "  optimizer.step() #更新参数\n",
        "\n",
        "  if (epoch+1) % 10 == 0:\n",
        "    print(f'Epoch [{epoch+1}/100],Loss: {loss.item():.4f}')\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrPau7GDx5x_",
        "outputId": "21954d8a-146c-4bf5-9d98-33b2f451872d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100],Loss: 0.4096\n",
            "Epoch [20/100],Loss: 0.3979\n",
            "Epoch [30/100],Loss: 0.3871\n",
            "Epoch [40/100],Loss: 0.3771\n",
            "Epoch [50/100],Loss: 0.3679\n",
            "Epoch [60/100],Loss: 0.3595\n",
            "Epoch [70/100],Loss: 0.3517\n",
            "Epoch [80/100],Loss: 0.3442\n",
            "Epoch [90/100],Loss: 0.3369\n",
            "Epoch [100/100],Loss: 0.3299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = torch.tensor([[1,2,3],[4,5,6]], dtype=torch.float32)\n",
        "print(\"orign:\\n\",tensor)\n",
        "\n",
        "# 1. **索引和切片操作**\n",
        "print(\"\\n【索引和切片】\")\n",
        "print(\"first line:\",tensor[0])\n",
        "print(\"first line, first cloum:\",tensor[0,0])\n",
        "print(tensor[:,1]) # 获取第二列所有元素\n",
        "\n",
        "# 2. **形状变换操作**\n",
        "print(\"\\n【形状变换】\")\n",
        "reshaped = tensor.view(3,2) # 改变张量形状为 3x2\n",
        "print(reshaped)\n",
        "flattened = tensor.flatten() # 将张量展平成一维\n",
        "print(flattened)\n",
        "\n",
        "# 3. **数学运算操作**\n",
        "print(\"\\n【数学运算】\")\n",
        "tensor_add = tensor + 10\n",
        "print('add:',tensor_add)\n",
        "tensor_mul = tensor * 2\n",
        "print('mul:',tensor_mul)\n",
        "tensor_sum = tensor.sum()\n",
        "print('sum:',tensor_sum)\n",
        "\n",
        "# 4. **与其他张量的操作**\n",
        "print(\"\\n【与其他张量操作】\")\n",
        "tensor2 = torch.tensor([[1,1,1],[2,2,2]],dtype=torch.float32)\n",
        "print('tensor2:',tensor2)\n",
        "tensor_dot = torch.matmul(tensor,tensor2.t())\n",
        "print('matmul:',tensor_dot)\n",
        "\n",
        "# 5. **条件判断和筛选**\n",
        "print(\"\\n【条件判断和筛选】\")\n",
        "mask = tensor > 3\n",
        "print('mask:',mask)\n",
        "filter_tensor = tensor[tensor > 4]\n",
        "print('filter:',filter_tensor)"
      ],
      "metadata": {
        "id": "Cc6aWtXK4XcL",
        "outputId": "a3b3c532-46cb-4dc3-f7ae-015220a79bb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "orign:\n",
            " tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.]])\n",
            "\n",
            "【索引和切片】\n",
            "first line: tensor([1., 2., 3.])\n",
            "first line, first cloum: tensor(1.)\n",
            "tensor([2., 5.])\n",
            "\n",
            "【形状变换】\n",
            "tensor([[1., 2.],\n",
            "        [3., 4.],\n",
            "        [5., 6.]])\n",
            "tensor([1., 2., 3., 4., 5., 6.])\n",
            "\n",
            "【数学运算】\n",
            "add: tensor([[11., 12., 13.],\n",
            "        [14., 15., 16.]])\n",
            "mul: tensor([[ 2.,  4.,  6.],\n",
            "        [ 8., 10., 12.]])\n",
            "sum: tensor(21.)\n",
            "\n",
            "【与其他张量操作】\n",
            "tensor2: tensor([[1., 1., 1.],\n",
            "        [2., 2., 2.]])\n",
            "matmul: tensor([[ 6., 12.],\n",
            "        [15., 30.]])\n",
            "\n",
            "【条件判断和筛选】\n",
            "mask: tensor([[False, False, False],\n",
            "        [ True,  True,  True]])\n",
            "filter: tensor([5., 6.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "前馈神经网络（Feedforward Neural Network，FNN）\n",
        "前馈神经网络（Feedforward Neural Network，FNN）是神经网络家族中的基本单元。\n",
        "\n",
        "前馈神经网络特点是数据从输入层开始，经过一个或多个隐藏层，最后到达输出层，全过程没有循环或反馈。\n",
        "\n",
        "循环神经网络（Recurrent Neural Network, RNN）\n",
        "循环神经网络（Recurrent Neural Network, RNN）络是一类专门处理序列数据的神经网络，能够捕获输入数据中时间或顺序信息的依赖关系。\n",
        "\n",
        "RNN 的特别之处在于它具有\"记忆能力\"，可以在网络的隐藏状态中保存之前时间步的信息。\n",
        "\n",
        "循环神经网络用于处理随时间变化的数据模式。\n",
        "\n",
        "在 RNN 中，相同的层被用来接收输入参数，并在指定的神经网络中显示输出参数。\n",
        "\n",
        "PyTorch 提供了许多常见的神经网络层，以下是几个常见的：\n",
        "\n",
        "nn.Linear(in_features, out_features)：全连接层，输入 in_features 个特征，输出 out_features 个特征。\n",
        "nn.Conv2d(in_channels, out_channels, kernel_size)：2D 卷积层，用于图像处理。\n",
        "nn.MaxPool2d(kernel_size)：2D 最大池化层，用于降维。\n",
        "nn.ReLU()：ReLU 激活函数，常用于隐藏层。\n",
        "nn.Softmax(dim)：Softmax 激活函数，通常用于输出层，适用于多类分类问题。\n",
        "\n",
        "\n",
        "激活函数（Activation Function）\n",
        "激活函数决定了神经元是否应该被激活。它们是非线性函数，使得神经网络能够学习和执行更复杂的任务。常见的激活函数包括：\n",
        "\n",
        "Sigmoid：用于二分类问题，输出值在 0 和 1 之间。\n",
        "Tanh：输出值在 -1 和 1 之间，常用于输出层之前。\n",
        "ReLU（Rectified Linear Unit）：目前最流行的激活函数之一，定义为 f(x) = max(0, x)，有助于解决梯度消失问题。\n",
        "Softmax：常用于多分类问题的输出层，将输出转换为概率分布。\n",
        "\n",
        "损失函数（Loss Function）\n",
        "损失函数用于衡量模型的预测值与真实值之间的差异。\n",
        "\n",
        "常见的损失函数包括：\n",
        "\n",
        "均方误差（MSELoss）：回归问题常用，计算输出与目标值的平方差。\n",
        "交叉熵损失（CrossEntropyLoss）：分类问题常用，计算输出和真实标签之间的交叉熵。\n",
        "BCEWithLogitsLoss：二分类问题，结合了 Sigmoid 激活和二元交叉熵损失。\n",
        "\n",
        "优化器（Optimizer）\n",
        "优化器负责在训练过程中更新网络的权重和偏置。\n",
        "\n",
        "常见的优化器包括：\n",
        "\n",
        "SGD（随机梯度下降）\n",
        "Adam（自适应矩估计）\n",
        "RMSprop（均方根传播）\n",
        "\n",
        "训练过程（Training Process）\n",
        "训练神经网络涉及以下步骤：\n",
        "\n",
        "准备数据：通过 DataLoader 加载数据。\n",
        "定义损失函数和优化器。\n",
        "前向传播：计算模型的输出。\n",
        "计算损失：与目标进行比较，得到损失值。\n",
        "反向传播：通过 loss.backward() 计算梯度。\n",
        "更新参数：通过 optimizer.step() 更新模型的参数。\n",
        "重复上述步骤，直到达到预定的训练轮数。\n",
        "\n",
        "测试与评估\n",
        "训练完成后，需要对模型进行测试和评估。\n",
        "\n",
        "常见的步骤包括：\n",
        "\n",
        "计算测试集的损失：测试模型在未见过的数据上的表现。\n",
        "计算准确率（Accuracy）：对于分类问题，计算正确预测的比例。\n",
        "\n",
        "\n",
        "神经网络类型\n",
        "前馈神经网络（Feedforward Neural Networks）：数据单向流动，从输入层到输出层，无反馈连接。\n",
        "卷积神经网络（Convolutional Neural Networks, CNNs）：适用于图像处理，使用卷积层提取空间特征。\n",
        "循环神经网络（Recurrent Neural Networks, RNNs）：适用于序列数据，如时间序列分析和自然语言处理，允许信息反馈循环。\n",
        "长短期记忆网络（Long Short-Term Memory, LSTM）：一种特殊的RNN，能够学习长期依赖关系。"
      ],
      "metadata": {
        "id": "y6uF-K1D9dTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 自定义数据集类\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, X_data, Y_data):\n",
        "        \"\"\"\n",
        "        初始化数据集，X_data 和 Y_data 是两个列表或数组\n",
        "        X_data: 输入特征\n",
        "        Y_data: 目标标签\n",
        "        \"\"\"\n",
        "        self.X_data = X_data\n",
        "        self.Y_data = Y_data\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"返回数据集的大小\"\"\"\n",
        "        return len(self.X_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"返回指定索引的数据\"\"\"\n",
        "        x = torch.tensor(self.X_data[idx], dtype=torch.float32)  # 转换为 Tensor\n",
        "        y = torch.tensor(self.Y_data[idx], dtype=torch.float32)\n",
        "        return x, y\n",
        "\n",
        "# 示例数据\n",
        "X_data = [[1, 2], [3, 4], [5, 6], [7, 8]]  # 输入特征\n",
        "Y_data = [1, 0, 1, 0]  # 目标标签\n",
        "\n",
        "# 创建数据集实例\n",
        "dataset = MyDataset(X_data, Y_data)\n",
        "\n",
        "# 创建 DataLoader 实例，batch_size 设置每次加载的样本数量\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "# 打印加载的数据\n",
        "for epoch in range(1):\n",
        "    for batch_idx, (inputs, labels) in enumerate(dataloader):\n",
        "        print(f'Batch {batch_idx + 1}:')\n",
        "        print(f'Inputs: {inputs}')\n",
        "        print(f'Labels: {labels}')"
      ],
      "metadata": {
        "id": "35wIFWvpA-84",
        "outputId": "6c05c98b-eec2-4f5f-d43e-95d5745cdafb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1:\n",
            "Inputs: tensor([[5., 6.],\n",
            "        [3., 4.]])\n",
            "Labels: tensor([1., 0.])\n",
            "Batch 2:\n",
            "Inputs: tensor([[1., 2.],\n",
            "        [7., 8.]])\n",
            "Labels: tensor([1., 0.])\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}